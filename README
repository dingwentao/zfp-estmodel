INTRODUCTION

  This is zfp 0.1.0, an open source C++ library for compressed floating-point
  arrays that support both read and write random access.  zfp was written by
  Peter Lindstrom at Lawrence Livermore National Laboratory, and is based on
  the algorithm described in the following paper:

    Peter Lindstrom
    "Fixed-Rate Compressed Floating-Point Arrays"
    IEEE Transactions on Visualization and Computer Graphics,
      20(12):2674-2683, December 2014.
    doi:10.1109/TVCG.2014.2346458

  zfp was designed to achieve high compression ratios of floating-point
  data, and therefore employs lossy but optionally error-bounded compression.
  Although bit-for-bit lossless compression is not always possible, zfp can
  approach almost perfectly lossless compression for most data sets.

  zfp works best for 2D and 3D arrays that exhibit spatial coherence.
  Although zfp also provides a 1D array class that can be used for 1D
  signals or even unstructured floating-point streams, the compression
  scheme has not been well optimized for this use case, and rate and quality
  are not likely to be competitive with floating-point compressors designed
  for 1D streams.

  zfp is freely available as open source under a BSD license, as outlined in
  the file 'LICENSE'.  For information on the API and general usage, please
  see the file 'API' in this directory.


INSTALLATION

  The main compressor and array classes are implemented entirely in header
  files, and therefore no installation is necessary.  To compile the code
  examples on Linux or OS X, type

    cd examples; make

  See below for more details on these example programs.


ALGORITHM OVERVIEW

  The zfp lossy compression scheme is based on the idea of breaking a
  d-dimensional array into independent blocks of 4^d values, e.g. 4x4x4
  values in three dimensions.  Each block is compressed/decompressed
  entirely independently from all other blocks.  In this sense, zfp is
  similar to current texture compression schemes for image coding
  implemented in hardware on graphics cards and mobile devices.

  The zfp compression scheme is based on three components, outlined below.

  (1) The d-dimensional array is partitioned into blocks of dimensions 4^d.
  The independent floating-point values in a block are converted to what is
  known as a block-floating-point representation, which uses a single, common
  floating-point exponent for all 4^d values.  The effect of this conversion
  is to turn each floating-point value into a 32- or 64-bit signed integer.

  (2) The integers are decorrelated using a custom, high-speed, orthogonal
  transform, much in the spirit of the discrete cosine transform used in
  JPEG image coding.  The transform exploits separability and is implemented
  efficiently using the lifting scheme.  If the data is "smooth," then this
  transform will turn most integers into small signed values clustered
  around zero.

  (3) The transform coefficients are compressed using embedded coding by
  exploiting the property that most coefficients have many leading zeros.
  This coder emits one bit at a time, with each successive bit potentially
  improving the quality of the reconstructed signal.  The early bits are
  most important and have the greatest impact on signal quality, with the
  last few bits providing very small changes.  The resulting compressed
  bit stream can be truncated at any point and still allow for a valid
  reconstruction.  This property is the basis for the fixed-rate functionality
  and random access to fixed-size blocks supported by zfp.

  Various parameters are exposed for controlling the quality and compressed
  size of a block, and can be specified by the user at a very fine
  granularity.  These parameters are discussed below.


CODE EXAMPLES

  The 'examples' directory includes two programs that make use of the
  compressor.  The 'diffusion' example is a simple forward Euler solver
  for the heat equation on a 2D regular grid, and is intended to show
  how to declare and work with zfp's compressed arrays, as well as give
  an idea of how changing the zfp precision affects the error in the
  solution.  It is possible to compile this code with conventional
  uncompressed arrays for comparison.  The usage is:

    diffusion [precision] [nx] [ny] [nt]

  where 'precision' specifies the exact number of bits to store per
  floating-point value (default = 64); 'nx' and 'ny' specify the grid size
  (default = 100x100); and 'nt' specifies the number of time steps to run
  (the default is to run until time t = 1).

  The 'zfp' program is intended for evaluating the rate-distortion
  (compression rate and quality) provided by the compressor.  It takes
  a raw array of floats or doubles as input, and optionally outputs the
  reconstructed array obtained after lossy compression followed by
  decompression.  Various statistics on compression rate and error are
  also displayed.

  zfp takes three arguments for array dimensions.  For 2D arrays, specify
  nz = 0; for 1D arrays, set ny = nz = 0.  Note that if nz is set to one
  for a 2D array, then the data will be treated as 3D and padded, which
  may negatively impact the compression rate.

  In addition to the array dimensions, zfp accepts four constraint
  parameters that, together, can be used to achieve various effects.
  These constraints are:

    minbits: the minimum number of bits used to represent a block
    maxbits: the maximum number of bits used to represent a block
    maxprec: the maximum number of bit planes encoded
    minexp:  the smallest bit plane number encoded

  Bit plane e here refers to the bits in a data set whose place value is
  2^e.  For instance, in single-precision floating-point bit planes -149
  through 127 are supported (when also counting denormalized numbers).

  Care must be taken to allow all constraints to be met, as encoding
  stops as soon as a single constraint is violated.  The effects of
  the above four parameters are best explained in terms of the three main
  compression modes supported by zfp (see ALGORITHM OVERVIEW above for
  additional details):

  Fixed rate:
    In fixed-rate mode, each block of 4^d values in d dimensions is stored
    using a fixed number of bits specified by the user.  This is achieved
    by setting minbits = maxbits, maxprec = 0, and minexp = -1024.  The
    fixed-rate mode is needed to support random access to blocks.  Note
    that the amortized number of bits used per value is maxbits / 4^d.

  Fixed precision:
    In fixed-precision mode, the number of bits used to encode a block may
    vary, but the number of bit planes (i.e. the precision) encoded for the
    transform coefficients is fixed.  This mode is achieved by specifying
    the precision in maxprec and fully relaxing the size constraints, i.e.
    minbits = 0, maxbits = UINT_MAX, and minexp = -1024.  Fixed-precision
    mode allows bounding the relative error.

  Fixed accuracy:
    In fixed-accuracy mode, all transform coefficient bit planes up to a
    minimum bit plane number are encoded.  The actual minimum bit plane
    is not necessarily minexp, but depends on the dimensionality of the
    data.  The reason for this is that the orthogonal transform incurs
    range expansion, and the amount of expansion depends on the number of
    dimensions.  Thus, minexp should be interpreted as the base-2 logarithm
    of an absolute error tolerance.  In other words, given an uncompressed
    value f and a reconstructed value g, the absolute difference |f - g|
    is guaranteed to be at most 2^minexp.  Note that this error tolerance
    is not always tight (especially for 3D arrays), but can conservatively
    be set so that even for worst-case inputs the error tolerance is
    respected.  To achieve fixed accuracy to within 'tolerance', set
    minexp = floor(log2(tolerance)), minbits = 0, maxbits = UINT_MAX, and
    minprec = 0.  As in fixed-precision mode, the number of bits used per
    block is not fixed but dictated by the data.

  As mentioned above, other combinations of constraints can be used.
  For example, to ensure that the compressed stream is not larger than
  the uncompressed one, or that it fits within the amount of memory
  allocated, one may in conjunction with other constraints set
  maxbits = 4^d * CHAR_BIT * sizeof(Type), where Type is either float or
  double.  The minbits parameter is useful only in fixed-rate mode--it
  ensures that zero-bits are padded to blocks that compress to fewer
  than minbits = maxbits bits.


VERSIONS

  zfp 0.1.0, November 12, 2014

    - Initial beta release.


LIMITATIONS AND MISSING FEATURES

  zfp is released as a beta version, with the intent of giving people access
  to the code and soliciting feedback on how to improve zfp for the first
  full release.  As such, the zfp API is experimental and has not been
  fixed, and it is entirely possible that future versions will employ a
  different API.

  Below is a list of known limitations and desirable features that may make
  it into future versions of zfp.

  - The current version of zfp allows for near lossless compression through
    suitable parameter choices, but no guarantees are made that bit-for-bit
    lossless compression is achieved.  We envision supporting lossless
    compression in a future version by compressing the difference between
    the original data and nearly losslessly compressed data.

  - Special values like infinity and NaN are not supported.  Denorms are
    correctly handled.

  - Currently no compressed format exists for storing compressed arrays
    externally, e.g. on disk.  Although it is possible to compress data
    to memory and write the compressed stream out, no meta data is
    embedded in the stream, and it is entirely up to the user to ensure
    that the data is read back in and interpreted according to the original
    array dimensions and floating-point type.  A compressed format is in
    development.

  - The compressed array class does not provide functions for accessing
    the compressed representation.  Rather, one has to use the ZFP::Codec
    classes to get at the compressed data.

  - No iterators are provided for traversing an array, and currently one
    has to use integer indexing.  Performance could in cases be improved
    by limiting the traversal to sequential access.

  - There currently is no way to make a complete copy of a compressed
    array, i.e. a = b; does not work for arrays a and b.

  - zfp can potentially provide higher precision than conventional float
    and double arrays, but the interface currently does not expose this.
    For example, such added precision could be useful in finite difference
    computations, where catastrophic cancellation can be an issue when
    insufficient precision is available.

  - Only single and double precision types are supported.  Generalizations
    to IEEE half and quad precision would be useful.  For instance,
    compressed 64-bit-per-value storage of 128-bit quad precision numbers
    could greatly improve the accuracy of double-precision floating-point
    computations using the same amount of storage.


QUESTIONS, COMMENTS, AND BUG REPORTS

  For bug reports, questions, and suggestions for improvements, please
  contact Peter Lindstrom at pl@llnl.gov.
